apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.12/23"],"mac_address":"0a:58:0a:fe:00:0c","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.12/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.12"
            ],
            "mac": "0a:58:0a:fe:00:0c",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-gpu-feature-discovery
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: gpu-feature-discovery-
    labels:
      app: gpu-feature-discovery
      app.kubernetes.io/part-of: nvidia-gpu
      controller-revision-hash: 74f8c45f7f
      pod-template-generation: "1"
    name: gpu-feature-discovery-hnr8w
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: gpu-feature-discovery
      uid: 7131f395-982f-4229-ae76-15c4d0eb8594
    resourceVersion: "450480"
    uid: a97d1552-7762-4205-b13a-f80e962f14c9
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - command:
      - gpu-feature-discovery
      env:
      - name: GFD_SLEEP_INTERVAL
        value: 60s
      - name: GFD_FAIL_ON_INIT_ERROR
        value: "true"
      - name: NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: MIG_STRATEGY
        value: single
      - name: NVIDIA_MIG_MONITOR_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s-device-plugin@sha256:2d16df5f3f12081b4bd6b317cf697e5c7a195c53cec7e0bab756db02a06b985c
      imagePullPolicy: IfNotPresent
      name: gpu-feature-discovery
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: output-dir
      - mountPath: /sys
        name: host-sys
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpmlt
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpmlt
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.gpu-feature-discovery: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-gpu-feature-discovery
    serviceAccountName: nvidia-gpu-feature-discovery
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/node-feature-discovery/features.d
        type: ""
      name: output-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - name: kube-api-access-bpmlt
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [toolkit-validation]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [gpu-feature-discovery]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [gpu-feature-discovery]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/k8s-device-plugin@sha256:2d16df5f3f12081b4bd6b317cf697e5c7a195c53cec7e0bab756db02a06b985c
      imageID: ""
      lastState: {}
      name: gpu-feature-discovery
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /etc/kubernetes/node-feature-discovery/features.d
        name: output-dir
      - mountPath: /sys
        name: host-sys
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpmlt
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://7af55cc4cddf3c43f2c29d8f6f81aadb02791d7f65c5fac978be3c3a7e7eb5c5
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: toolkit-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bpmlt
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.12
    podIPs:
    - ip: 10.254.0.12
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      alm-examples: |-
        [
          {
            "apiVersion": "nvidia.com/v1",
            "kind": "ClusterPolicy",
            "metadata": {
              "name": "gpu-cluster-policy"
            },
            "spec": {
              "operator": {
                "defaultRuntime": "crio",
                "use_ocp_driver_toolkit": true,
                "initContainer": {
                }
              },
              "cdi": {
                "enabled": true
              },
              "sandboxWorkloads": {
                "enabled": false,
                "defaultWorkload": "container"
              },
              "driver": {
                "enabled": true,
                "useNvidiaDriverCRD": false,
                "kernelModuleType": "auto",
                "upgradePolicy": {
                  "autoUpgrade": true,
                  "drain": {
                    "deleteEmptyDir": false,
                    "enable": false,
                    "force": false,
                    "timeoutSeconds": 300
                  },
                  "maxParallelUpgrades": 1,
                  "maxUnavailable": "25%",
                  "podDeletion": {
                    "deleteEmptyDir": false,
                    "force": false,
                    "timeoutSeconds": 300
                  },
                  "waitForCompletion": {
                    "timeoutSeconds": 0
                  }
                },
                "repoConfig": {
                  "configMapName": ""
                },
                "certConfig": {
                  "name": ""
                },
                "licensingConfig": {
                  "nlsEnabled": true,
                  "secretName": ""
                },
                "virtualTopology": {
                  "config": ""
                },
                "kernelModuleConfig": {
                  "name": ""
                }
              },
              "dcgmExporter": {
                "enabled": true,
                "config": {
                  "name": ""
                },
                "serviceMonitor": {
                  "enabled": true
                }
              },
              "dcgm": {
                "enabled": true
              },
              "daemonsets": {
                "updateStrategy": "RollingUpdate",
                "rollingUpdate": {
                  "maxUnavailable": "1"
                }
              },
              "devicePlugin": {
                "enabled": true,
                "config": {
                  "name": "",
                  "default": ""
                },
                "mps": {
                  "root": "/run/nvidia/mps"
                }
              },
              "gfd": {
                "enabled": true
              },
              "migManager": {
                "enabled": true
              },
              "nodeStatusExporter": {
                "enabled": true
              },
              "mig": {
                "strategy": "single"
              },
              "toolkit": {
                "enabled": true
              },
              "validator": {
                "plugin": {
                  "env": []
                }
              },
              "vgpuManager": {
                "enabled": false
              },
              "vgpuDeviceManager": {
                "enabled": true
              },
              "sandboxDevicePlugin": {
                "enabled": true
              },
              "vfioManager": {
                "enabled": true
              },
              "gds": {
                "enabled": false
              },
              "gdrcopy": {
                "enabled": false
              }
            }
          },
          {
            "apiVersion": "nvidia.com/v1alpha1",
            "kind": "NVIDIADriver",
            "metadata": {
              "name": "gpu-driver"
            },
            "spec": {
              "driverType": "gpu",
              "repository": "nvcr.io/nvidia",
              "image": "driver",
              "version": "sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be",
              "nodeSelector": {},
              "manager": {},
              "repoConfig": {
                "name": ""
              },
              "certConfig": {
                "name": ""
              },
              "licensingConfig": {
                "nlsEnabled": true,
                "secretName": ""
              },
              "virtualTopologyConfig": {
                "name": ""
              },
              "kernelModuleConfig": {
                "name": ""
              }
            }
          }
        ]
      capabilities: Deep Insights
      categories: AI/Machine Learning, OpenShift Optional
      certified: "true"
      containerImage: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      createdAt: Fri Oct 24 13:56:11 PDT 2025
      description: Automate the management and monitoring of NVIDIA GPUs.
      features.operators.openshift.io/cnf: "false"
      features.operators.openshift.io/cni: "false"
      features.operators.openshift.io/csi: "false"
      features.operators.openshift.io/disconnected: "true"
      features.operators.openshift.io/fips-compliant: "false"
      features.operators.openshift.io/proxy-aware: "true"
      features.operators.openshift.io/tls-profiles: "false"
      features.operators.openshift.io/token-auth-aws: "false"
      features.operators.openshift.io/token-auth-azure: "false"
      features.operators.openshift.io/token-auth-gcp: "false"
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.15/23"],"mac_address":"0a:58:0a:fe:00:0f","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.15/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.15"
            ],
            "mac": "0a:58:0a:fe:00:0f",
            "default": true,
            "dns": {}
        }]
      olm.operatorGroup: nvidia-gpu-operator
      olm.operatorNamespace: nvidia-gpu-operator
      olm.skipRange: '>=1.9.0 <25.10.0'
      olm.targetNamespaces: nvidia-gpu-operator
      openshift.io/scc: hostmount-anyuid
      operatorframework.io/properties: '{"properties":[{"type":"olm.gvk","value":{"group":"nvidia.com","kind":"ClusterPolicy","version":"v1"}},{"type":"olm.gvk","value":{"group":"nvidia.com","kind":"NVIDIADriver","version":"v1alpha1"}},{"type":"olm.package","value":{"packageName":"gpu-operator-certified","version":"25.10.0"}}]}'
      operatorframework.io/suggested-namespace: nvidia-gpu-operator
      operators.operatorframework.io/builder: operator-sdk-v1.4.0
      operators.operatorframework.io/project_layout: go.kubebuilder.io/v3
      provider: NVIDIA
      repository: http://github.com/NVIDIA/gpu-operator
      support: NVIDIA
    creationTimestamp: "2025-11-21T08:51:37Z"
    generateName: gpu-operator-54d498cdbc-
    labels:
      app: gpu-operator
      app.kubernetes.io/component: gpu-operator
      nvidia.com/gpu-driver-upgrade-drain.skip: "true"
      pod-template-hash: 54d498cdbc
    name: gpu-operator-54d498cdbc-f2lft
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: gpu-operator-54d498cdbc
      uid: 9e0915ec-2bbf-4b96-bd86-b2e8e846b503
    resourceVersion: "449043"
    uid: bb22c64e-28f8-4bf6-95f9-495a380255ad
  spec:
    containers:
    - args:
      - --leader-elect
      - --leader-lease-renew-deadline
      - 60s
      command:
      - gpu-operator
      env:
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VALIDATOR_IMAGE
        value: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      - name: GFD_IMAGE
        value: nvcr.io/nvidia/k8s-device-plugin@sha256:2d16df5f3f12081b4bd6b317cf697e5c7a195c53cec7e0bab756db02a06b985c
      - name: CONTAINER_TOOLKIT_IMAGE
        value: nvcr.io/nvidia/k8s/container-toolkit@sha256:c0d14372bc5d74e882eb60c5c5580459675bf08a2e4c627422cc949af6e9717d
      - name: DCGM_IMAGE
        value: nvcr.io/nvidia/cloud-native/dcgm@sha256:d42cd2afe032d9bfcb101cc2f739683b123a6c744f2732f221362a3cac776806
      - name: DCGM_EXPORTER_IMAGE
        value: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:ed2dfcb708949de649ab8e1b23521cfd1eba89774dbbe662b6835f1ffcaadb1a
      - name: DEVICE_PLUGIN_IMAGE
        value: nvcr.io/nvidia/k8s-device-plugin@sha256:2d16df5f3f12081b4bd6b317cf697e5c7a195c53cec7e0bab756db02a06b985c
      - name: DRIVER_IMAGE
        value: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      - name: DRIVER_IMAGE-570
        value: nvcr.io/nvidia/driver@sha256:ef9856ae8491b376364df1f0cad388bcf9983bd2b6c78000523c13536cf828c3
      - name: DRIVER_IMAGE-535
        value: nvcr.io/nvidia/driver@sha256:35359117c5cdf786694d2fdba2ba038e7f673c5d0243c9ed4dc6cdaf6e675e4a
      - name: DRIVER_MANAGER_IMAGE
        value: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:a6c12abacc9c4f51d3653c90fcad32f19799069889338601407eba05fea4ba18
      - name: MIG_MANAGER_IMAGE
        value: nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:9194a84d3ff2d99886653add3867ec8ee03755442a6d75844932812a12a968de
      - name: CUDA_BASE_IMAGE
        value: nvcr.io/nvidia/cuda@sha256:d19fe621624c4eb6ac931b8558daa3ecc0c3f07f1e2a52e0267e083d22dceade
      - name: VFIO_MANAGER_IMAGE
        value: nvcr.io/nvidia/cuda@sha256:d19fe621624c4eb6ac931b8558daa3ecc0c3f07f1e2a52e0267e083d22dceade
      - name: SANDBOX_DEVICE_PLUGIN_IMAGE
        value: nvcr.io/nvidia/kubevirt-gpu-device-plugin@sha256:119de9a331a47203858b99901f44d0c4a8052961b4e60327f4b100d0ab8c9df0
      - name: VGPU_DEVICE_MANAGER_IMAGE
        value: nvcr.io/nvidia/cloud-native/vgpu-device-manager@sha256:098c01e11589e08ede9ffb3002d1c3dff424f0f0c5d1bfcbfd54a359073f16dd
      - name: GDRCOPY_IMAGE
        value: nvcr.io/nvidia/cloud-native/gdrdrv@sha256:5c4e61f7ba83d7a64ff2523d447c209ce5bde1ddc79acaf1f32f19620b4912d6
      - name: OPERATOR_CONDITION_NAME
        value: gpu-operator-certified.v25.10.0
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 15
        periodSeconds: 20
        successThreshold: 1
        timeoutSeconds: 1
      name: gpu-operator
      ports:
      - containerPort: 8080
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 200Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - MKNOD
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s7pg7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: sno-arm
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      seLinuxOptions:
        level: s0:c26,c25
    serviceAccount: gpu-operator
    serviceAccountName: gpu-operator
    terminationGracePeriodSeconds: 10
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - name: kube-api-access-s7pg7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:49:04Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:51:37Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:49:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:49:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:51:37Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://54425d136e350ee0ea9dd5a2baf57f6238873aba3726fc53f84db3f32f685f80
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: gpu-operator
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:49:03Z"
      volumeMounts:
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s7pg7
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    phase: Running
    podIP: 10.254.0.15
    podIPs:
    - ip: 10.254.0.15
    qosClass: Burstable
    startTime: "2025-11-21T08:51:37Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.8/23"],"mac_address":"0a:58:0a:fe:00:08","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.8/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.8"
            ],
            "mac": "0a:58:0a:fe:00:08",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: privileged
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: nvidia-container-toolkit-daemonset-
    labels:
      app: nvidia-container-toolkit-daemonset
      controller-revision-hash: 79bfbcf65f
      pod-template-generation: "1"
    name: nvidia-container-toolkit-daemonset-sxfkr
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-container-toolkit-daemonset
      uid: 3f9f2dda-4a98-494c-ba87-48576a01ac20
    resourceVersion: "450488"
    uid: 169ead87-4be2-4345-88e6-496d616a5f2d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - args:
      - /bin/entrypoint.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: ROOT
        value: /usr/local/nvidia
      - name: NVIDIA_CONTAINER_RUNTIME_MODES_CDI_DEFAULT_KIND
        value: management.nvidia.com/gpu
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      - name: TOOLKIT_PID_FILE
        value: /run/nvidia/toolkit/toolkit.pid
      - name: CRIO_CONFIG_MODE
        value: hook
      - name: RUNTIME
        value: crio
      - name: RUNTIME_CONFIG
        value: /runtime/config-dir/config.toml
      - name: CRIO_CONFIG
        value: /runtime/config-dir/config.toml
      - name: RUNTIME_DROP_IN_CONFIG
        value: /runtime/config-dir.d/99-nvidia.conf
      - name: RUNTIME_DROP_IN_CONFIG_HOST_PATH
        value: /etc/crio/crio.conf.d/99-nvidia.conf
      image: nvcr.io/nvidia/k8s/container-toolkit@sha256:c0d14372bc5d74e882eb60c5c5580459675bf08a2e4c627422cc949af6e9717d
      imagePullPolicy: IfNotPresent
      name: nvidia-container-toolkit-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bin/entrypoint.sh
        name: nvidia-container-toolkit-entrypoint
        readOnly: true
        subPath: entrypoint.sh
      - mountPath: /run/nvidia/toolkit
        name: toolkit-root
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /usr/local/nvidia
        name: toolkit-install-dir
      - mountPath: /usr/share/containers/oci/hooks.d
        name: crio-hooks
      - mountPath: /driver-root
        mountPropagation: HostToContainer
        name: driver-install-dir
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /var/run/cdi
        name: cdi-root
      - mountPath: /runtime/config-dir/
        name: crio-config
      - mountPath: /runtime/config-dir.d/
        name: crio-drop-in-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rg84v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostPID: true
    initContainers:
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "true"
      - name: COMPONENT
        value: driver
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: driver-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/driver
        mountPropagation: HostToContainer
        name: driver-install-dir
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /host-dev-char
        name: host-dev-char
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rg84v
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.container-toolkit: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-container-toolkit
    serviceAccountName: nvidia-container-toolkit
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 448
        name: nvidia-container-toolkit-entrypoint
      name: nvidia-container-toolkit-entrypoint
    - hostPath:
        path: /run/nvidia/toolkit
        type: DirectoryOrCreate
      name: toolkit-root
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /run/nvidia/driver
        type: DirectoryOrCreate
      name: driver-install-dir
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /usr/local/nvidia
        type: ""
      name: toolkit-install-dir
    - hostPath:
        path: /run/containers/oci/hooks.d
        type: ""
      name: crio-hooks
    - hostPath:
        path: /dev/char
        type: ""
      name: host-dev-char
    - hostPath:
        path: /var/run/cdi
        type: DirectoryOrCreate
      name: cdi-root
    - hostPath:
        path: /etc/crio
        type: DirectoryOrCreate
      name: crio-config
    - hostPath:
        path: /etc/crio/crio.conf.d
        type: DirectoryOrCreate
      name: crio-drop-in-config
    - name: kube-api-access-rg84v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [driver-validation]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-container-toolkit-ctr]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-container-toolkit-ctr]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/k8s/container-toolkit@sha256:c0d14372bc5d74e882eb60c5c5580459675bf08a2e4c627422cc949af6e9717d
      imageID: ""
      lastState: {}
      name: nvidia-container-toolkit-ctr
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /bin/entrypoint.sh
        name: nvidia-container-toolkit-entrypoint
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/nvidia/toolkit
        name: toolkit-root
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /usr/local/nvidia
        name: toolkit-install-dir
      - mountPath: /usr/share/containers/oci/hooks.d
        name: crio-hooks
      - mountPath: /driver-root
        name: driver-install-dir
      - mountPath: /host
        name: host-root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/cdi
        name: cdi-root
      - mountPath: /runtime/config-dir/
        name: crio-config
      - mountPath: /runtime/config-dir.d/
        name: crio-drop-in-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rg84v
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://9ec8204af5026a12f069064317024a1429feab9d5db51f46dc5bae2624a26b74
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: driver-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /run/nvidia/driver
        name: driver-install-dir
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /host
        name: host-root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /host-dev-char
        name: host-dev-char
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rg84v
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.8
    podIPs:
    - ip: 10.254.0.8
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.39/23"],"mac_address":"0a:58:0a:fe:00:27","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.39/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.39"
            ],
            "mac": "0a:58:0a:fe:00:27",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-operator-validator
    creationTimestamp: "2025-11-21T08:53:41Z"
    generateName: nvidia-cuda-validator-
    labels:
      app: nvidia-cuda-validator
    name: nvidia-cuda-validator-8b5vw
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: nvidia.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ClusterPolicy
      name: gpu-cluster-policy
      uid: 31ff37ae-5d30-433e-af73-ae6d1e5a5daf
    resourceVersion: "447678"
    uid: 990b170e-1f31-4ac1-9cee-2a0184ba7966
  spec:
    containers:
    - args:
      - echo cuda workload validation is successful
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: nvidia-cuda-validator
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4k7s2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - vectorAdd
      command:
      - sh
      - -c
      env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: cuda-validation
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4k7s2
        readOnly: true
    nodeName: sno-arm
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-operator-validator
    serviceAccountName: nvidia-operator-validator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-4k7s2
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:53:45Z"
      status: "False"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:53:43Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:53:41Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:53:41Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:53:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://3d47a3237c50b485e1956e96f72b08fc0c18954b036da171cdc1aab840cbac3c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: nvidia-cuda-validator
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://3d47a3237c50b485e1956e96f72b08fc0c18954b036da171cdc1aab840cbac3c
          exitCode: 0
          finishedAt: "2025-11-21T08:53:43Z"
          reason: Completed
          startedAt: "2025-11-21T08:53:43Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4k7s2
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://111b565e5cf832769db1926d92265945f1f761d13fbb6cd22101739cabd48bb2
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: cuda-validation
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://111b565e5cf832769db1926d92265945f1f761d13fbb6cd22101739cabd48bb2
          exitCode: 0
          finishedAt: "2025-11-21T08:53:42Z"
          reason: Completed
          startedAt: "2025-11-21T08:53:42Z"
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4k7s2
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Succeeded
    qosClass: BestEffort
    startTime: "2025-11-21T08:53:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.7/23"],"mac_address":"0a:58:0a:fe:00:07","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.7/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.7"
            ],
            "mac": "0a:58:0a:fe:00:07",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: lvms-vgmanager
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: nvidia-dcgm-exporter-
    labels:
      app: nvidia-dcgm-exporter
      controller-revision-hash: 6bd699447c
      pod-template-generation: "1"
    name: nvidia-dcgm-exporter-n6jqs
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dcgm-exporter
      uid: 51e683d8-60d6-4e41-843b-b5ffbd4b7965
    resourceVersion: "450482"
    uid: 09f6ea49-4943-4655-88ba-5921997027b3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - env:
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_COLLECTORS
        value: /etc/dcgm-exporter/dcp-metrics-included.csv
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: DCGM_REMOTE_HOSTENGINE_INFO
        value: nvidia-dcgm:5555
      image: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:ed2dfcb708949de649ab8e1b23521cfd1eba89774dbbe662b6835f1ffcaadb1a
      imagePullPolicy: IfNotPresent
      name: nvidia-dcgm-exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvrf4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvrf4
        readOnly: true
    - command:
      - /bin/entrypoint.sh
      env:
      - name: NVIDIA_DISABLE_REQUIRE
        value: "true"
      image: nvcr.io/nvidia/cuda@sha256:d19fe621624c4eb6ac931b8558daa3ecc0c3f07f1e2a52e0267e083d22dceade
      imagePullPolicy: IfNotPresent
      name: init-pod-nvidia-node-status-exporter
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
      - mountPath: /bin/entrypoint.sh
        name: init-config
        readOnly: true
        subPath: entrypoint.sh
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvrf4
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.dcgm-exporter: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000700000
      seLinuxOptions:
        level: s0:c26,c25
    serviceAccount: nvidia-dcgm-exporter
    serviceAccountName: nvidia-dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - hostPath:
        path: /run/nvidia
        type: ""
      name: run-nvidia
    - configMap:
        defaultMode: 448
        name: nvidia-dcgm-exporter
      name: init-config
    - name: kube-api-access-qvrf4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [toolkit-validation init-pod-nvidia-node-status-exporter]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-dcgm-exporter]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-dcgm-exporter]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:ed2dfcb708949de649ab8e1b23521cfd1eba89774dbbe662b6835f1ffcaadb1a
      imageID: ""
      lastState: {}
      name: nvidia-dcgm-exporter
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvrf4
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://fdefe3d8a0e7c5d47d8a39b395c196e35522d9d299e7b24e4b83487ebaca700a
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: toolkit-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvrf4
        readOnly: true
        recursiveReadOnly: Disabled
    - image: nvcr.io/nvidia/cuda@sha256:d19fe621624c4eb6ac931b8558daa3ecc0c3f07f1e2a52e0267e083d22dceade
      imageID: ""
      lastState: {}
      name: init-pod-nvidia-node-status-exporter
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
      - mountPath: /bin/entrypoint.sh
        name: init-config
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvrf4
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.7
    podIPs:
    - ip: 10.254.0.7
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.10/23"],"mac_address":"0a:58:0a:fe:00:0a","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.10/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.10"
            ],
            "mac": "0a:58:0a:fe:00:0a",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-dcgm
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: nvidia-dcgm-
    labels:
      app: nvidia-dcgm
      controller-revision-hash: 7c69b7ddc8
      pod-template-generation: "1"
    name: nvidia-dcgm-m25d9
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-dcgm
      uid: b77e8ab6-4076-45cc-9be6-a035b72e6005
    resourceVersion: "450485"
    uid: 4a912749-713f-43b0-9209-7651b4043792
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - image: nvcr.io/nvidia/cloud-native/dcgm@sha256:d42cd2afe032d9bfcb101cc2f739683b123a6c744f2732f221362a3cac776806
      imagePullPolicy: IfNotPresent
      name: nvidia-dcgm-ctr
      ports:
      - containerPort: 5555
        name: dcgm
        protocol: TCP
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nf9fv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nf9fv
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.dcgm: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-dcgm
    serviceAccountName: nvidia-dcgm
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - name: kube-api-access-nf9fv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [toolkit-validation]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-dcgm-ctr]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-dcgm-ctr]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/cloud-native/dcgm@sha256:d42cd2afe032d9bfcb101cc2f739683b123a6c744f2732f221362a3cac776806
      imageID: ""
      lastState: {}
      name: nvidia-dcgm-ctr
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nf9fv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://6b99c5fc291abc51da432664590d0d8f582e8fa0fbc8e8bc1854687a33aa06e1
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: toolkit-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nf9fv
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.10
    podIPs:
    - ip: 10.254.0.10
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.11/23"],"mac_address":"0a:58:0a:fe:00:0b","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.11/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.11"
            ],
            "mac": "0a:58:0a:fe:00:0b",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: privileged
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: nvidia-device-plugin-daemonset-
    labels:
      app: nvidia-device-plugin-daemonset
      controller-revision-hash: 5d47d55b7b
      pod-template-generation: "1"
    name: nvidia-device-plugin-daemonset-v46gq
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-device-plugin-daemonset
      uid: 2db884fa-7ade-43fc-a8b3-7bd081e5cc82
    resourceVersion: "450489"
    uid: 807d00b3-bd46-4f5d-a7dd-71a354fc2d68
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - args:
      - /bin/entrypoint.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: PASS_DEVICE_SPECS
        value: "true"
      - name: FAIL_ON_INIT_ERROR
        value: "true"
      - name: DEVICE_LIST_STRATEGY
        value: envvar
      - name: DEVICE_ID_STRATEGY
        value: uuid
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: all
      - name: MPS_ROOT
        value: /run/nvidia/mps
      - name: GDRCOPY_ENABLED
        value: "true"
      - name: MIG_STRATEGY
        value: single
      - name: NVIDIA_MIG_MONITOR_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s-device-plugin@sha256:2d16df5f3f12081b4bd6b317cf697e5c7a195c53cec7e0bab756db02a06b985c
      imagePullPolicy: IfNotPresent
      name: nvidia-device-plugin
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bin/entrypoint.sh
        name: nvidia-device-plugin-entrypoint
        readOnly: true
        subPath: entrypoint.sh
      - mountPath: /var/lib/kubelet/device-plugins
        name: device-plugin
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /driver-root
        mountPropagation: HostToContainer
        name: driver-install-dir
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /var/run/cdi
        name: cdi-root
      - mountPath: /dev/shm
        name: mps-shm
      - mountPath: /mps
        name: mps-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qjhtj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container stack to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: HostToContainer
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qjhtj
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.device-plugin: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-device-plugin
    serviceAccountName: nvidia-device-plugin
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 448
        name: nvidia-device-plugin-entrypoint
      name: nvidia-device-plugin-entrypoint
    - hostPath:
        path: /var/lib/kubelet/device-plugins
        type: ""
      name: device-plugin
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /run/nvidia/driver
        type: DirectoryOrCreate
      name: driver-install-dir
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /var/run/cdi
        type: DirectoryOrCreate
      name: cdi-root
    - hostPath:
        path: /run/nvidia/mps
        type: DirectoryOrCreate
      name: mps-root
    - hostPath:
        path: /run/nvidia/mps/shm
        type: ""
      name: mps-shm
    - name: kube-api-access-qjhtj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [toolkit-validation]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-device-plugin]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-device-plugin]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/k8s-device-plugin@sha256:2d16df5f3f12081b4bd6b317cf697e5c7a195c53cec7e0bab756db02a06b985c
      imageID: ""
      lastState: {}
      name: nvidia-device-plugin
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /bin/entrypoint.sh
        name: nvidia-device-plugin-entrypoint
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/lib/kubelet/device-plugins
        name: device-plugin
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /driver-root
        name: driver-install-dir
      - mountPath: /host
        name: host-root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/cdi
        name: cdi-root
      - mountPath: /dev/shm
        name: mps-shm
      - mountPath: /mps
        name: mps-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qjhtj
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://b9bde363829b20f503bae7ed3348ce3ab15c8b714e91cc62f52e04cf05ff9810
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: toolkit-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qjhtj
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.11
    podIPs:
    - ip: 10.254.0.11
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.6/23"],"mac_address":"0a:58:0a:fe:00:06","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.6/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.6"
            ],
            "mac": "0a:58:0a:fe:00:06",
            "default": true,
            "dns": {}
        }]
      kubectl.kubernetes.io/default-container: nvidia-driver-ctr
      openshift.io/scc: nvidia-driver
    creationTimestamp: "2025-11-21T11:50:11Z"
    generateName: nvidia-driver-daemonset-418.94.202510230424-0-
    labels:
      app: nvidia-driver-daemonset-418.94.202510230424-0
      app.kubernetes.io/component: nvidia-driver
      controller-revision-hash: 864cc98c78
      nvidia.com/precompiled: "false"
      openshift.driver-toolkit: "true"
      pod-template-generation: "1"
    name: nvidia-driver-daemonset-418.94.202510230424-0-564xm
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-driver-daemonset-418.94.202510230424-0
      uid: 6dee0c70-dd26-4e2d-a49f-122252eb080c
    resourceVersion: "489816"
    uid: 1942cff1-835a-490a-85f6-fba09e4415fd
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/component
              operator: In
              values:
              - nvidia-driver
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - nv-ctr-run-with-dtk
      command:
      - ocp_dtk_entrypoint
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NODE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      - name: KERNEL_MODULE_TYPE
        value: auto
      - name: GPU_DIRECT_RDMA_ENABLED
        value: "true"
      - name: OPENSHIFT_VERSION
        value: "4.18"
      image: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - rm -f /run/nvidia/validations/.driver-ctr-ready
      name: nvidia-driver-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      startupProbe:
        exec:
          command:
          - sh
          - -c
          - nvidia-smi && touch /run/nvidia/validations/.driver-ctr-ready
        failureThreshold: 120
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 60
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: run-nvidia
      - mountPath: /run/nvidia-fabricmanager
        name: run-nvidia-fabricmanager
      - mountPath: /run/nvidia-topologyd
        name: run-nvidia-topologyd
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /run/mellanox/drivers/usr/src
        mountPropagation: HostToContainer
        name: mlnx-ofed-usr-src
      - mountPath: /run/mellanox/drivers
        mountPropagation: HostToContainer
        name: run-mellanox-drivers
      - mountPath: /sys/devices/system/memory/auto_online_blocks
        name: sysfs-memory-online
      - mountPath: /sys/module/firmware_class/parameters/path
        name: firmware-search-path
      - mountPath: /lib/firmware
        name: nv-firmware
      - mountPath: /etc/pki/ca-trust/extracted/pem
        name: gpu-operator-trusted-ca
        readOnly: true
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
    - args:
      - reload_nvidia_peermem
      command:
      - nvidia-driver
      image: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - sh
          - -c
          - nvidia-driver probe_nvidia_peermem
        failureThreshold: 1
        initialDelaySeconds: 30
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 10
      name: nvidia-peermem-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      startupProbe:
        exec:
          command:
          - sh
          - -c
          - nvidia-driver probe_nvidia_peermem
        failureThreshold: 120
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: run-nvidia
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
        readOnly: true
      - mountPath: /run/mellanox/drivers
        mountPropagation: HostToContainer
        name: run-mellanox-drivers
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
    - args:
      - gdrcopy-ctr-run-with-dtk
      command:
      - ocp_dtk_entrypoint
      image: nvcr.io/nvidia/cloud-native/gdrdrv@sha256:5c4e61f7ba83d7a64ff2523d447c209ce5bde1ddc79acaf1f32f19620b4912d6
      imagePullPolicy: IfNotPresent
      name: nvidia-gdrcopy-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      startupProbe:
        exec:
          command:
          - sh
          - -c
          - lsmod | grep gdrdrv
        failureThreshold: 120
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
        readOnly: true
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
    - args:
      - until [ -f /mnt/shared-nvidia-driver-toolkit/dir_prepared ]; do echo  Waiting
        for nvidia-driver-ctr container to prepare the shared directory ...; sleep
        10; done; exec /mnt/shared-nvidia-driver-toolkit/ocp_dtk_entrypoint dtk-build-driver
      command:
      - bash
      - -xc
      env:
      - name: RHCOS_VERSION
        value: 418.94.202510230424-0
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      - name: GDRCOPY_ENABLED
        value: "true"
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:75798070ee8bba72891c1ef66858117d352b2b98a4072c4ce5e6424d03845930
      imagePullPolicy: IfNotPresent
      name: openshift-driver-toolkit-ctr
      resources: {}
      securityContext:
        privileged: true
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/log
        name: var-log
      - mountPath: /run/mellanox/drivers/usr/src
        mountPropagation: HostToContainer
        name: mlnx-ofed-usr-src
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostPID: true
    initContainers:
    - args:
      - uninstall_driver
      command:
      - driver-manager
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      - name: ENABLE_GPU_POD_EVICTION
        value: "true"
      - name: ENABLE_AUTO_DRAIN
        value: "false"
      - name: DRAIN_USE_FORCE
        value: "false"
      - name: DRAIN_POD_SELECTOR_LABEL
      - name: DRAIN_TIMEOUT_SECONDS
        value: 0s
      - name: DRAIN_DELETE_EMPTYDIR_DATA
        value: "false"
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: GPU_DIRECT_RDMA_ENABLED
        value: "true"
      image: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:a6c12abacc9c4f51d3653c90fcad32f19799069889338601407eba05fea4ba18
      imagePullPolicy: IfNotPresent
      name: k8s-driver-manager
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: Bidirectional
        name: run-nvidia
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /sys
        name: host-sys
      - mountPath: /run/mellanox/drivers
        mountPropagation: HostToContainer
        name: run-mellanox-drivers
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      feature.node.kubernetes.io/system-os_release.OSTREE_VERSION: 418.94.202510230424-0
      nvidia.com/gpu.deploy.driver: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-driver
    serviceAccountName: nvidia-driver
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: DirectoryOrCreate
      name: run-nvidia
    - hostPath:
        path: /var/log
        type: ""
      name: var-log
    - hostPath:
        path: /dev/log
        type: ""
      name: dev-log
    - hostPath:
        path: /etc/os-release
        type: ""
      name: host-os-release
    - hostPath:
        path: /run/nvidia-fabricmanager
        type: DirectoryOrCreate
      name: run-nvidia-fabricmanager
    - hostPath:
        path: /run/nvidia-topologyd
        type: DirectoryOrCreate
      name: run-nvidia-topologyd
    - hostPath:
        path: /run/mellanox/drivers/usr/src
        type: DirectoryOrCreate
      name: mlnx-ofed-usr-src
    - hostPath:
        path: /run/mellanox/drivers
        type: DirectoryOrCreate
      name: run-mellanox-drivers
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /sys
        type: Directory
      name: host-sys
    - hostPath:
        path: /sys/module/firmware_class/parameters/path
        type: ""
      name: firmware-search-path
    - hostPath:
        path: /sys/devices/system/memory/auto_online_blocks
        type: ""
      name: sysfs-memory-online
    - hostPath:
        path: /run/nvidia/driver/lib/firmware
        type: DirectoryOrCreate
      name: nv-firmware
    - configMap:
        defaultMode: 420
        items:
        - key: ca-bundle.crt
          path: tls-ca-bundle.pem
        name: gpu-operator-trusted-ca
      name: gpu-operator-trusted-ca
    - emptyDir: {}
      name: shared-nvidia-driver-toolkit
    - name: kube-api-access-mkcfz
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:06Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:05Z"
      message: 'containers with unready status: [nvidia-driver-ctr nvidia-peermem-ctr
        nvidia-gdrcopy-ctr]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:05Z"
      message: 'containers with unready status: [nvidia-driver-ctr nvidia-peermem-ctr
        nvidia-gdrcopy-ctr]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:05Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://209a872a95a2450afb414a8b20e33f013ee33dc83edc290367fe2620cff719f8
      image: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      imageID: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      lastState:
        terminated:
          containerID: cri-o://209a872a95a2450afb414a8b20e33f013ee33dc83edc290367fe2620cff719f8
          exitCode: 1
          finishedAt: "2025-11-21T15:15:44Z"
          reason: Error
          startedAt: "2025-11-21T15:15:39Z"
      name: nvidia-driver-ctr
      ready: false
      restartCount: 44
      started: false
      state:
        waiting:
          message: back-off 5m0s restarting failed container=nvidia-driver-ctr pod=nvidia-driver-daemonset-418.94.202510230424-0-564xm_nvidia-gpu-operator(1942cff1-835a-490a-85f6-fba09e4415fd)
          reason: CrashLoopBackOff
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /run/nvidia-fabricmanager
        name: run-nvidia-fabricmanager
      - mountPath: /run/nvidia-topologyd
        name: run-nvidia-topologyd
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/mellanox/drivers/usr/src
        name: mlnx-ofed-usr-src
      - mountPath: /run/mellanox/drivers
        name: run-mellanox-drivers
      - mountPath: /sys/devices/system/memory/auto_online_blocks
        name: sysfs-memory-online
      - mountPath: /sys/module/firmware_class/parameters/path
        name: firmware-search-path
      - mountPath: /lib/firmware
        name: nv-firmware
      - mountPath: /etc/pki/ca-trust/extracted/pem
        name: gpu-operator-trusted-ca
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://38445f1b5b69f12f3492bcfff62c205d2cdafc862aff52a4207c8127b88a20f5
      image: nvcr.io/nvidia/cloud-native/gdrdrv@sha256:5c4e61f7ba83d7a64ff2523d447c209ce5bde1ddc79acaf1f32f19620b4912d6
      imageID: nvcr.io/nvidia/cloud-native/gdrdrv@sha256:3ddcd5b56e80a41e2176aab1663700e0576082dee84363a577cee8903a7e0eb7
      lastState:
        terminated:
          containerID: cri-o://a4cfd686ef1aac9fc7144e11b59e1b408369ccc62dcfbf8fb71198428a054080
          exitCode: 143
          finishedAt: "2025-11-21T15:12:05Z"
          reason: Error
          startedAt: "2025-11-21T14:52:05Z"
      name: nvidia-gdrcopy-ctr
      ready: false
      restartCount: 10
      started: false
      state:
        running:
          startedAt: "2025-11-21T15:12:05Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://fcd14a302c9ea4e80597f47540ce0dcbf7a0f4489499878c6ab166da549044ab
      image: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      imageID: nvcr.io/nvidia/driver@sha256:317c7ab01f28c87dc8c209b38c49fb2758a595d9ffbc1d71e18530788dcf34be
      lastState:
        terminated:
          containerID: cri-o://faef90d06318b66a0ed5a11a56d2cb8777ba58fe4b3798d5fccd880ddefc62d8
          exitCode: 143
          finishedAt: "2025-11-21T15:12:25Z"
          reason: Error
          startedAt: "2025-11-21T14:52:25Z"
      name: nvidia-peermem-ctr
      ready: false
      restartCount: 10
      started: false
      state:
        running:
          startedAt: "2025-11-21T15:12:25Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /var/log
        name: var-log
      - mountPath: /dev/log
        name: dev-log
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/mellanox/drivers
        name: run-mellanox-drivers
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
        recursiveReadOnly: Disabled
    - containerID: cri-o://77e6e55ca75e8a5a8bff0bae820da9c95f9b6c880c4aa7bd52751106d848914a
      image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:75798070ee8bba72891c1ef66858117d352b2b98a4072c4ce5e6424d03845930
      imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:470966c00cb3258fa78d122ed19971a385539435801ab7eb1ea133d173a9f8b7
      lastState: {}
      name: openshift-driver-toolkit-ctr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /mnt/shared-nvidia-driver-toolkit
        name: shared-nvidia-driver-toolkit
      - mountPath: /var/log
        name: var-log
      - mountPath: /run/mellanox/drivers/usr/src
        name: mlnx-ofed-usr-src
      - mountPath: /host-etc/os-release
        name: host-os-release
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://3ce7d83f3988bb0ed91b3df36ff7c721c745fd5a6363d3b134a3615676c41842
      image: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:a6c12abacc9c4f51d3653c90fcad32f19799069889338601407eba05fea4ba18
      imageID: nvcr.io/nvidia/cloud-native/k8s-driver-manager@sha256:53afb576497a9b6e909a3b6a35698f81532a35f5669f21b022eeebdb508fb244
      lastState: {}
      name: k8s-driver-manager
      ready: true
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: cri-o://3ce7d83f3988bb0ed91b3df36ff7c721c745fd5a6363d3b134a3615676c41842
          exitCode: 0
          finishedAt: "2025-11-21T11:51:41Z"
          reason: Completed
          startedAt: "2025-11-21T11:51:06Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /host
        name: host-root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /sys
        name: host-sys
      - mountPath: /run/mellanox/drivers
        name: run-mellanox-drivers
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mkcfz
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Running
    podIP: 10.254.0.6
    podIPs:
    - ip: 10.254.0.6
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:05Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.9/23"],"mac_address":"0a:58:0a:fe:00:09","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.9/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.9"
            ],
            "mac": "0a:58:0a:fe:00:09",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-mig-manager
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: nvidia-mig-manager-
    labels:
      app: nvidia-mig-manager
      controller-revision-hash: 78cb59cf74
      pod-template-generation: "1"
    name: nvidia-mig-manager-w5799
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-mig-manager
      uid: 7e133b56-0635-433b-add9-cc270d8e5386
    resourceVersion: "450479"
    uid: 5030f7f0-bbfe-43ca-950b-909b54916a84
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - args:
      - /bin/entrypoint.sh
      command:
      - /bin/sh
      - -c
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CONFIG_FILE
        value: /mig-parted-config/config.yaml
      - name: GPU_CLIENTS_FILE
        value: /gpu-clients/clients.yaml
      - name: DEFAULT_GPU_CLIENTS_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:9194a84d3ff2d99886653add3867ec8ee03755442a6d75844932812a12a968de
      imagePullPolicy: IfNotPresent
      name: nvidia-mig-manager
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /bin/entrypoint.sh
        name: nvidia-mig-manager-entrypoint
        readOnly: true
        subPath: entrypoint.sh
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /sys
        name: host-sys
      - mountPath: /mig-parted-config
        name: mig-parted-config
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
      - mountPath: /gpu-clients
        name: gpu-clients
      - mountPath: /driver-root
        mountPropagation: HostToContainer
        name: driver-install-dir
      - mountPath: /var/run/cdi
        name: cdi-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l2z8t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostIPC: true
    hostPID: true
    initContainers:
    - args:
      - until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia
        container toolkit to be setup; sleep 5; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: HostToContainer
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l2z8t
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.mig-manager: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-mig-manager
    serviceAccountName: nvidia-mig-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 448
        name: nvidia-mig-manager-entrypoint
      name: nvidia-mig-manager-entrypoint
    - hostPath:
        path: /sys
        type: Directory
      name: host-sys
    - configMap:
        defaultMode: 420
        name: default-mig-parted-config
      name: mig-parted-config
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /run/nvidia/driver
        type: DirectoryOrCreate
      name: driver-install-dir
    - hostPath:
        path: /
        type: ""
      name: host-root
    - configMap:
        defaultMode: 420
        name: default-gpu-clients
      name: gpu-clients
    - hostPath:
        path: /var/run/cdi
        type: DirectoryOrCreate
      name: cdi-root
    - name: kube-api-access-l2z8t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [toolkit-validation]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-mig-manager]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-mig-manager]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/cloud-native/k8s-mig-manager@sha256:9194a84d3ff2d99886653add3867ec8ee03755442a6d75844932812a12a968de
      imageID: ""
      lastState: {}
      name: nvidia-mig-manager
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /bin/entrypoint.sh
        name: nvidia-mig-manager-entrypoint
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /sys
        name: host-sys
      - mountPath: /mig-parted-config
        name: mig-parted-config
      - mountPath: /host
        name: host-root
      - mountPath: /gpu-clients
        name: gpu-clients
      - mountPath: /driver-root
        name: driver-install-dir
      - mountPath: /var/run/cdi
        name: cdi-root
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l2z8t
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://912e273a17d6816146d8f3634af5495567fc9907cdc30fa9caf285ed9425377e
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: toolkit-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l2z8t
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.9
    podIPs:
    - ip: 10.254.0.9
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.27/23"],"mac_address":"0a:58:0a:fe:00:1b","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.27/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.27"
            ],
            "mac": "0a:58:0a:fe:00:1b",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: lvms-vgmanager
    creationTimestamp: "2025-11-21T08:51:39Z"
    generateName: nvidia-node-status-exporter-
    labels:
      app: nvidia-node-status-exporter
      controller-revision-hash: 6d4bbcd578
      pod-template-generation: "1"
    name: nvidia-node-status-exporter-cd78t
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-node-status-exporter
      uid: 066a7eb7-046f-4725-a65a-d739d001f9d9
    resourceVersion: "448646"
    uid: eb35364c-cc31-448c-84cd-bee5e99cea4f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - command:
      - nvidia-validator
      env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: void
      - name: COMPONENT
        value: metrics
      - name: METRICS_PORT
        value: "8000"
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: nvidia-node-status-exporter
      ports:
      - containerPort: 8000
        name: node-status
        protocol: TCP
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia
        mountPropagation: HostToContainer
        name: run-nvidia
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j4p4v
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.node-status-exporter: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1000700000
      seLinuxOptions:
        level: s0:c26,c25
    serviceAccount: nvidia-node-status-exporter
    serviceAccountName: nvidia-node-status-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia
        type: Directory
      name: run-nvidia
    - hostPath:
        path: /
        type: ""
      name: host-root
    - name: kube-api-access-j4p4v
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:49:03Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:51:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:49:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:49:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T08:51:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: cri-o://872655673c0746554b8663cff0699342d3e553308903f1a8dc7208b3e0672613
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: nvidia-node-status-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:49:02Z"
      volumeMounts:
      - mountPath: /run/nvidia
        name: run-nvidia
      - mountPath: /host
        name: host-root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-j4p4v
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    phase: Running
    podIP: 10.254.0.27
    podIPs:
    - ip: 10.254.0.27
    qosClass: BestEffort
    startTime: "2025-11-21T08:51:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.254.0.13/23"],"mac_address":"0a:58:0a:fe:00:0d","gateway_ips":["10.254.0.1"],"routes":[{"dest":"10.254.0.0/16","nextHop":"10.254.0.1"},{"dest":"172.31.0.0/16","nextHop":"10.254.0.1"},{"dest":"169.254.0.5/32","nextHop":"10.254.0.1"},{"dest":"100.64.0.0/16","nextHop":"10.254.0.1"}],"ip_address":"10.254.0.13/23","gateway_ip":"10.254.0.1","role":"primary"}}'
      k8s.v1.cni.cncf.io/network-status: |-
        [{
            "name": "ovn-kubernetes",
            "interface": "eth0",
            "ips": [
                "10.254.0.13"
            ],
            "mac": "0a:58:0a:fe:00:0d",
            "default": true,
            "dns": {}
        }]
      openshift.io/scc: nvidia-operator-validator
    creationTimestamp: "2025-11-21T11:51:41Z"
    generateName: nvidia-operator-validator-
    labels:
      app: nvidia-operator-validator
      app.kubernetes.io/part-of: gpu-operator
      controller-revision-hash: 7bbf747564
      pod-template-generation: "1"
    name: nvidia-operator-validator-6fhjb
    namespace: nvidia-gpu-operator
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: nvidia-operator-validator
      uid: 3918a277-4b80-4177-8df6-fd6a21bb3d57
    resourceVersion: "450481"
    uid: 2a0f9b28-845a-4c08-801e-68e0ca76f667
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - sno-arm
    containers:
    - args:
      - echo all validations are successful; while true; do sleep 86400; done
      command:
      - sh
      - -c
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      lifecycle:
        preStop:
          exec:
            command:
            - sh
            - -c
            - rm -f /run/nvidia/validations/*-ready
      name: nvidia-operator-validator
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "true"
      - name: COMPONENT
        value: driver
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: driver-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host
        mountPropagation: HostToContainer
        name: host-root
        readOnly: true
      - mountPath: /run/nvidia/driver
        mountPropagation: HostToContainer
        name: driver-install-dir
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /host-dev-char
        name: host-dev-char
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "true"
      - name: COMPONENT
        value: gdrcopy
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: gdrcopy-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
        seLinuxOptions:
          level: s0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: WITH_WAIT
        value: "false"
      - name: COMPONENT
        value: toolkit
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: toolkit-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: WITH_WAIT
        value: "false"
      - name: COMPONENT
        value: cuda
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VALIDATOR_IMAGE
        value: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      - name: VALIDATOR_IMAGE_PULL_POLICY
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: cuda-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
    - args:
      - nvidia-validator
      command:
      - sh
      - -c
      env:
      - name: COMPONENT
        value: plugin
      - name: WITH_WAIT
        value: "false"
      - name: WITH_WORKLOAD
        value: "false"
      - name: MIG_STRATEGY
        value: single
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: OPERATOR_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: VALIDATOR_IMAGE
        value: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      - name: VALIDATOR_IMAGE_PULL_POLICY
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imagePullPolicy: IfNotPresent
      name: plugin-validation
      resources: {}
      securityContext:
        privileged: true
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /run/nvidia/validations
        mountPropagation: Bidirectional
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
    nodeName: sno-arm
    nodeSelector:
      nvidia.com/gpu.deploy.operator-validator: "true"
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: nvidia-operator-validator
    serviceAccountName: nvidia-operator-validator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /run/nvidia/validations
        type: DirectoryOrCreate
      name: run-nvidia-validations
    - hostPath:
        path: /run/nvidia/driver
        type: ""
      name: driver-install-dir
    - hostPath:
        path: /
        type: ""
      name: host-root
    - hostPath:
        path: /dev/char
        type: ""
      name: host-dev-char
    - name: kube-api-access-t6vdv
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
        - configMap:
            items:
            - key: service-ca.crt
              path: service-ca.crt
            name: openshift-service-ca.crt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:43Z"
      status: "True"
      type: PodReadyToStartContainers
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with incomplete status: [driver-validation gdrcopy-validation
        toolkit-validation cuda-validation plugin-validation]'
      reason: ContainersNotInitialized
      status: "False"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-operator-validator]'
      reason: ContainersNotReady
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      message: 'containers with unready status: [nvidia-operator-validator]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2025-11-21T11:51:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: ""
      lastState: {}
      name: nvidia-operator-validator
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
        recursiveReadOnly: Disabled
    hostIP: 192.168.44.65
    hostIPs:
    - ip: 192.168.44.65
    initContainerStatuses:
    - containerID: cri-o://5c194a2f202b905756a32ee5b3331e6992b614fbd3b15cb7591f9eace22342dc
      image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: nvcr.io/nvidia/gpu-operator@sha256:04d31f9babb5567fe34d1b68124cf55fb5cddcf1e8c2d8a40fa5154d221a24ce
      lastState: {}
      name: driver-validation
      ready: false
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2025-11-21T11:51:42Z"
      volumeMounts:
      - mountPath: /host
        name: host-root
        readOnly: true
        recursiveReadOnly: Disabled
      - mountPath: /run/nvidia/driver
        name: driver-install-dir
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /host-dev-char
        name: host-dev-char
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
        recursiveReadOnly: Disabled
    - image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: ""
      lastState: {}
      name: gdrcopy-validation
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
        recursiveReadOnly: Disabled
    - image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: ""
      lastState: {}
      name: toolkit-validation
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
        recursiveReadOnly: Disabled
    - image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: ""
      lastState: {}
      name: cuda-validation
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
        recursiveReadOnly: Disabled
    - image: nvcr.io/nvidia/gpu-operator@sha256:d4841412c9b8d27c53b1588dcebffc5451e9ab0fd36b2c656c33a65356507cfd
      imageID: ""
      lastState: {}
      name: plugin-validation
      ready: false
      restartCount: 0
      started: false
      state:
        waiting:
          reason: PodInitializing
      volumeMounts:
      - mountPath: /run/nvidia/validations
        name: run-nvidia-validations
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-t6vdv
        readOnly: true
        recursiveReadOnly: Disabled
    phase: Pending
    podIP: 10.254.0.13
    podIPs:
    - ip: 10.254.0.13
    qosClass: BestEffort
    startTime: "2025-11-21T11:51:41Z"
kind: List
metadata:
  resourceVersion: ""
